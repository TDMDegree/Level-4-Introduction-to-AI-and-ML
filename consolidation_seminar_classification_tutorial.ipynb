{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOO6B5utMtI3E3FJXUCTry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TDMDegree/Level-4-Introduction-to-AI-and-ML/blob/main/consolidation_seminar_classification_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Introduction\n",
        "\n",
        "The adult dataset is from the 1994 Census database. Details of this dataset can be found at the UCI Machine Learning Repository. My main aim is to create a model to get a prediction of whether their income will exceed $50,000. The objectives that I will follow to complete my main aim are set out below:\n",
        "\n",
        "1) Clean the data: Replace or delete missing data.\n",
        "\n",
        "2) Explore the data: Delete any unnecessary fields\n",
        "\n",
        "3) Scale the data\n",
        "\n",
        "4)Convert the needed data for modelling.\n",
        "\n",
        "5) Explore the machine learning algorithms.\n",
        "\n",
        "6)Optimise the selected machine learning algorithm.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wDyL1bgwCMpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Stage 1: Exploratory Data Analysis (EDA)\n",
        "\n",
        "Task 1 - Present the code that tells me all of the columns, data types, and records\n",
        "\n",
        "Importantly, you should begin considering whether the dimensionality of the dataset is high or low. Additionally, you should strive to understand the information described by all the columns.\n"
      ],
      "metadata": {
        "id": "EvPmBi-9CPQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1 - upload the Salary.txt file and turn it into a pandas dataframe.\n"
      ],
      "metadata": {
        "id": "XAdTW-SbCgfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the information collected on the dataframe, we know that there are 14 features that could be used for predicting the label column. However, from the original 15 columns, 2 columns are not self-explanatory. Both fnlwgt and education-num does not have an obvious correlation to the salary. After further examining the information on the dataset, fnlwgt is a weight on the sample and education-num is a repetitive column repeating the information in the education columns. It was decided that both columns would be dropped from the dataframe."
      ],
      "metadata": {
        "id": "hm-4PO09KnPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 - drop the following columns from the dataframe :\"fnlwgt\", \"education-num\"\n"
      ],
      "metadata": {
        "id": "hYCjqe25Kpvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "From the information file on the dataset, we know that all missing data was represented as \"?\".\n",
        "\n",
        "All missing data was changed to the numpy form \"NaN\".\n"
      ],
      "metadata": {
        "id": "lIXEU6OKLM0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 - replace the ? with pd.Na\n",
        "\n"
      ],
      "metadata": {
        "id": "NW8dxTaALN4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A report on the missing data was then produced"
      ],
      "metadata": {
        "id": "seFXO7MyLUua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4 - check to see if we have any missing values\n"
      ],
      "metadata": {
        "id": "cQ5CmP2QLVOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More information was gathered to find out how much data would be lost if we deleted all the missing data."
      ],
      "metadata": {
        "id": "_WZa-JX3Lry_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5 - check to see how many records we will lost by creating 2 dataframes. One that drops all missing values and the other that doesn't. Compare the number of records in each dataframe.\n"
      ],
      "metadata": {
        "id": "B7xxzR0ALtDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we would lose only 2399 records out of 32561, further exploration of the data could result in a reduction in the loss of data.\n",
        "\n",
        "Importantly, there are several strategies that could be used to address this missing data. Before making a decision, I need to understand the significance of the missing records and the feature itself.\n",
        "\n",
        "**Exploring the data**"
      ],
      "metadata": {
        "id": "7X2lG7rpYpGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 6- Find out the value count for each of the columns that have missing records\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jLm4mjyCYo7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "From the information above, we can see that 1 unique value of this feature has over 91% of the data. Therefore, a decision was made to change all missing data from the Native Country column to equal \"United States\".\n",
        "\n",
        "*Importantly, this was not the only handling strategy that could have been used. I could have tried to explore the correlation between country and salary to determine if I could have removed this feature completely or created a ML classification to predict the most likely country based on the other features. *\n",
        "\n",
        "Additional, the majority of the other unique values had such a low number of records, resulting in a high cardinality for those records. The decision was made to try and combine the countries together in a way that could add to the model. The decision was to use the information from the website :\n",
        "\n",
        "https://www.nationsonline.org/oneworld/GNI_PPP_of_countries.htm\n",
        "\n",
        "to split the countries into high_PPP ,medium_PPP and low_PPP.\n",
        "\n",
        "Task 7 - Replace all of np.NaN with United-States and loop round each of the arrays to replace based on high_PPP ,medium_PPP and low_PPP\n",
        "\n",
        "low_PPP = [\" Honduras\", \" Vietnam\",\" Cambodia\",\" Laos\",\" Haiti\", \" Yugoslavia\",\" India\",\" Guatemala\", \" Nicaragua\"]\n",
        "\n",
        "medium_PPP = [\" Trinadad&Tobago\",\" Poland\" ,\" Mexico\" , \" Thailand\",\" Iran\",\" Columbia\", \" Peru\", \" Philippines\" ,\" China\",\" Ecuador\" , \" Cuba\",\" El-Salvador\",\" Jamaica\",\" South\"]\n",
        "\n",
        "high_PPP = [\" Holand-Netherlands\",\" Scotland\",\" Ireland\",\" Hong\",\" Beligum\" ,\" Japan\",\" Italy\",\" England\",\" Germany\",\" Canada\",\" France\",\" Taiwan\",\" Greece\",\" Portugal\" , \" Hungary\",\" Outlying-US(Guam-USVI-etc)\", \" Puerto-Rico\", \" Dominican-Republic\"]\n",
        "\n",
        "Importantly, this was not the only handling strategy that could have been used. I could have keep the data as it was and evaluated the different models to select the most suitable one.\n"
      ],
      "metadata": {
        "id": "LQmOeg_1KodU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[\"native-country\"] = df[\"native-country\"].replace(pd.NA, \" United-States\")\n",
        "low_PPP = [\" Honduras\", \" Vietnam\",\" Cambodia\",\" Laos\",\" Haiti\",\n",
        "               \" Yugoslavia\",\" India\",\" Guatemala\", \" Nicaragua\"]\n",
        "medium_PPP = [\" Trinadad&Tobago\",\" Poland\" ,\" Mexico\" , \" Thailand\",\" Iran\",\n",
        "                \" Columbia\", \" Peru\", \" Philippines\" ,\" China\",\" Ecuador\" ,\n",
        "                \" Cuba\",\" El-Salvador\",\" Jamaica\",\" South\"]\n",
        "high_PPP = [\" Holand-Netherlands\",\" Scotland\",\" Ireland\",\" Hong\",\" Beligum\" ,\" Japan\",\" Italy\",\" England\",\" Germany\",\" Canada\",\" France\",\" Taiwan\",\" Greece\",\n",
        "               \" Portugal\" , \" Hungary\",\" Outlying-US(Guam-USVI-etc)\",\n",
        "               \" Puerto-Rico\", \" Dominican-Republic\"]\n",
        "\n",
        "for i in high_PPP:\n",
        "    df[\"native-country\"] = df[\"native-country\"].replace(i,\"high_PPP\")\n",
        "\n",
        "for i in medium_PPP:\n",
        "    df[\"native-country\"] = df[\"native-country\"].replace(i,\"medium_PPP\")\n",
        "\n",
        "for i in low_PPP:\n",
        "    df[\"native-country\"] = df[\"native-country\"].replace(i,\"  low_PPP\")\n",
        "\n",
        "print(df['native-country'].value_counts())"
      ],
      "metadata": {
        "id": "zTywQao9KoBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e2eba5-ee48-4fac-b183-9bd9e9691312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "native-country\n",
            " United-States    29753\n",
            "medium_PPP         1536\n",
            "high_PPP            897\n",
            "  low_PPP           375\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step would be to look at the working class and occupation data and how the data is distributed among the unique values. Does it have high cardinality ? What should I do with the missing values ?\n",
        "\n",
        "For the rest of the columns, I will drop all missing values"
      ],
      "metadata": {
        "id": "lDl3N6EMLtfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#task 11 - Drop all missing values\n"
      ],
      "metadata": {
        "id": "EaFat_3uMYb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Encoding the data\n",
        "\n",
        "The data has been cleaned and combined to make it more effective in the modeling process.However, currently there are still a number of columns that have an unsuitable datatype for some of the machine learning algorithms that I will be using.\n",
        "\n",
        "I will be converting all the object datatypes to a numerical type. The options that I have for this are using the Labelencoder and OneHotEncoder.\n",
        "\n",
        "Due to the non-ordinal relations between the data categories in each column. I have chosen to use pandas.get_dummies on the object datatypes -\n",
        "\n",
        "[\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\" were converted to a numeric value.]\n"
      ],
      "metadata": {
        "id": "bx5GRkNATb2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#task 12 - encode the following columns \"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\" using the pd.get_dummies() method\n"
      ],
      "metadata": {
        "id": "gmK5Xd5lTyE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These features are then added to the original dataframe, and the original columns deleted."
      ],
      "metadata": {
        "id": "1xq62RWOT2CZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Continuing with the necessary conversion of the data, the label column was updated so that:\n",
        "\n",
        "0 = Below $50,000\n",
        "\n",
        "1 = Above $50,000\n"
      ],
      "metadata": {
        "id": "OD2kob6UUKqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 13 - replace the label with a 1 for over 50k and a 0 for under\n",
        "\n"
      ],
      "metadata": {
        "id": "tCTsDA1tUOww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection\n",
        "\n",
        "Feature selection was then conducted to identify any highly or lowly correlated relationships between the label and the different variables. This process aims to reduce the dimensions of the dataset.\n",
        "\n",
        "No features were removed; however, justification should be provided regarding why this ML model was selected based on these findings.\n",
        "\n",
        "Importantly, this was just start and should have continued further for a more detailed correlation investigation"
      ],
      "metadata": {
        "id": "hf-h1MXQpRaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 - find out if capital-gain and age have any correlation\n"
      ],
      "metadata": {
        "id": "nO-a4hSVp7j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, I droped the label so I can start scaling and normalising the features."
      ],
      "metadata": {
        "id": "Zw7vH_aBqies"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 - put the label column into its own dataframe and get rid of the label column in the main one\n",
        "\n"
      ],
      "metadata": {
        "id": "Kyy_Vgtgp1WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the Data\n",
        "\n",
        "\n",
        "\n",
        "The features were then scaled due to the differences between distributions of the features.\n",
        "\n",
        "Importantly, I should really prove this by showing the distributions of the different features and discussing how it would impact the different ML models\n",
        "\n",
        "The MinMaxScaler was used to transforms the data.\n"
      ],
      "metadata": {
        "id": "jgdoN27zUT7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16- scale the data\n"
      ],
      "metadata": {
        "id": "oljFdOrRUZo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Exploring the machine learning algorithms\n",
        "\n",
        "With the transformation of the data complete, a selection of algorithms were chosen to explore the best method to predict the salary classification. Below are the selected algorithms :\n",
        "\n",
        "1)Logestic Regression\n",
        "\n",
        "2)Random Forest Tree\n",
        "\n",
        "3)Extra Trees\n",
        "\n",
        "4)Support Vector Machine\n",
        "\n",
        "5)Neural Network\n"
      ],
      "metadata": {
        "id": "NKvM8LtSrG3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n"
      ],
      "metadata": {
        "id": "hngGPWplrMyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data was then divided into test and train datasets. We have 2 ways to divide our data, with the train_test_split option and cross_val_score. We will initially see the results from the test_train_split."
      ],
      "metadata": {
        "id": "Baux_ufpradV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#task 17 - split the data\n"
      ],
      "metadata": {
        "id": "Fht-N_NPrfT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifiers are then set to their default settings and each classifier is trained and tested with the data."
      ],
      "metadata": {
        "id": "v85rHv29rg8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    ExtraTreesClassifier(n_estimators=100),\n",
        "    svm.SVC(gamma='scale'),\n",
        "    RandomForestClassifier(n_estimators=100),\n",
        "    MLPClassifier(max_iter=1000),    LogisticRegression(max_iter=1000)\n",
        "]\n",
        "\n",
        "\n",
        "for clf in classifiers:\n",
        "    clf.fit(X_train,y_train)\n",
        "    name = clf.__class__.__name__\n",
        "    print(name)\n",
        "    prediction = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, prediction)\n",
        "    print(\"Acc : \",acc)\n",
        "    matrix = confusion_matrix(y_test, prediction)\n",
        "    print(matrix)"
      ],
      "metadata": {
        "id": "qVA8qm-2rjAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163ee448-741c-4ff9-8956-a57261d0cd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExtraTreesClassifier\n",
            "Acc :  0.8296875\n",
            "[[5205  577]\n",
            " [ 731 1167]]\n",
            "SVC\n",
            "Acc :  0.8358072916666667\n",
            "[[5354  428]\n",
            " [ 833 1065]]\n",
            "RandomForestClassifier\n",
            "Acc :  0.847265625\n",
            "[[5306  476]\n",
            " [ 697 1201]]\n",
            "MLPClassifier\n",
            "Acc :  0.8404947916666666\n",
            "[[5214  568]\n",
            " [ 657 1241]]\n",
            "LogisticRegression\n",
            "Acc :  0.8459635416666667\n",
            "[[5357  425]\n",
            " [ 758 1140]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As we can see,  all machine learning algorithms are above 80%. With so many algorithms having a similar accuracy rating, further investigation will be undertaken by looking at the confusion matrix.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VLTpx3RprsXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating a classification machine learning model, it's essential to use appropriate metrics that reflect its performance accurately. Metrics such as accuracy, precision, recall (sensitivity), and F1-score are suitable for assessing classification models. Accuracy measures the proportion of correctly classified instances out of the total instances, providing an overall view of the model's correctness. Precision quantifies the accuracy of positive predictions, while recall (sensitivity) measures the proportion of actual positives that are correctly identified by the model. The F1-score, which is the harmonic mean of precision and recall, balances these two metrics and is particularly useful in scenarios where there is an uneven class distribution.\n"
      ],
      "metadata": {
        "id": "yxu8I804oewK"
      }
    }
  ]
}