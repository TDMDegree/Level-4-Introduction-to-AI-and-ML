{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO6+iQtCLEjCwSsq97z5Pj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TDMDegree/Level-4-Introduction-to-AI-and-ML/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Introduction**\n",
        "\n",
        "The adult dataset is from the 1994 Census database. Details of this dataset can be found at the UCI Machine Learning Repository. My main aim is to create a simple GUI so that users can input their own data and get a prediction of whether their income will exceed $50,000. The objectives that I will follow to complete my main aim are set out below:\n",
        "\n",
        "    Clean the data: Replace or delete missing data.\n",
        "    Explore the data: Delete any unnecessary fields and reduce the variance of individual features.\n",
        "    Normalise the data: Convert the needed data for modelling.\n",
        "    Explore the machine learning algorithms.\n",
        "    Optimise the selected machine learning algorithm.\n",
        "    Create the GUI.\n"
      ],
      "metadata": {
        "id": "1tdoOXXvB7JQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1: Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "_h5BHf_PB_KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 - Present the code that tells me all of the columns, data types, and records\n",
        "\n",
        "Importantly, you should begin considering whether the dimensionality of the dataset is high or low. Additionally, you should strive to understand the information described by all the columns."
      ],
      "metadata": {
        "id": "G9NtzH0MChSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1 :\n",
        "# You will need to import the correct libraries , read the csv file Salary.txt and make sure that the columns have the following names\n",
        "#names = [ \"age\" ,\"workclass\", \"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\n",
        "#\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"label\"]"
      ],
      "metadata": {
        "id": "ERfjA111C1QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 - Which two columns do not explain the type of information they are storing? Can you delete these two columns?\n",
        "\n",
        "(Note: Make sure to identify and specify the columns that do not explain the type of information they are storing before deleting them.)"
      ],
      "metadata": {
        "id": "qSuxX1bRDgDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the code to drop the columns\n"
      ],
      "metadata": {
        "id": "JtwSMMGxHZVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 - From the information file on the dataset, we know that all missing data was represented as \"?\". Change this to NaN"
      ],
      "metadata": {
        "id": "Kv2mWPCyHlZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3 - Add the code to change all \"?\"\" to NaN . In my answer, I used numpy.NaN for this"
      ],
      "metadata": {
        "id": "kf0gjTslH00W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4 - Produce a report on the total number of null values for each column"
      ],
      "metadata": {
        "id": "U5KArw3DIDhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print all the columns and the total number of null values for each column"
      ],
      "metadata": {
        "id": "SltqSKX-a6ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5 - Before we delete these records, more information is needed to find out how much data would be lost if we deleted all the missing data."
      ],
      "metadata": {
        "id": "EFD43V4XbAvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create two dataframes. Drop the Null values from one of them and then compare the record\n",
        "#differences to see how many records would be deleted"
      ],
      "metadata": {
        "id": "jKvSq4XKa6bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Importantly, there are several strategies that could be used to address this missing data. Before making a decision, I need to understand the significance of the missing records and the feature itself.*"
      ],
      "metadata": {
        "id": "lj-hpdxtGaex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring the data**"
      ],
      "metadata": {
        "id": "bYRyybF8bgEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6 - Using the native-country column, where we could potentially drop the null value records and ...\n",
        "\n",
        "    1) Create a graph that shows the different values within that column and the value count for each value.\n",
        "    or\n",
        "    2) Show the percentage of each of the different values within that column."
      ],
      "metadata": {
        "id": "WQwGa6wccLtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete task 6"
      ],
      "metadata": {
        "id": "Ah4aE8hux7sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "From the information above, we can see that 1 unique value of this feature has over 91% of the data. Therefore, a decision was made to change all missing data from the Native Country column to equal \"United States\".\n",
        "\n",
        "***Importantly, this was not the only handling strategy that could have been used. I could have tried to explore the correlation between country and salary to determine if I could have removed this feature completely or created a ML classification to predict the most likely country based on the other features. ***\n",
        "\n",
        "Additional, the majority of the other unique values had such a low number of records, resulting in a high cardinality for those records. The decision was made to try and combine the countries together in a way that could add to the model. The decision was to use the information from the website :\n",
        "\n",
        "https://www.nationsonline.org/oneworld/GNI_PPP_of_countries.htm\n",
        "\n",
        "to split the countries into high_PPP ,medium_PPP and low_PPP.\n",
        "\n",
        "Task 7 - Replace all of np.NaN with United-States and loop round each of the arrays to replace based on high_PPP ,medium_PPP and low_PPP\n",
        "\n",
        "low_PPP = [\" Honduras\", \" Vietnam\",\" Cambodia\",\" Laos\",\" Haiti\",\n",
        "               \" Yugoslavia\",\" India\",\" Guatemala\", \" Nicaragua\"]\n",
        "\n",
        "\n",
        "medium_PPP = [\" Trinadad&Tobago\",\" Poland\" ,\" Mexico\" , \" Thailand\",\" Iran\",\" Columbia\", \" Peru\", \" Philippines\" ,\" China\",\" Ecuador\" ,\n",
        "         \" Cuba\",\" El-Salvador\",\" Jamaica\",\" South\"]\n",
        "\n",
        "high_PPP = [\" Holand-Netherlands\",\" Scotland\",\" Ireland\",\" Hong\",\" Beligum\" ,\" Japan\",\" Italy\",\" England\",\" Germany\",\" Canada\",\" France\",\" Taiwan\",\" Greece\",\" Portugal\" , \" Hungary\",\" Outlying-US(Guam-USVI-etc)\", \" Puerto-Rico\", \" Dominican-Republic\"]\n",
        "\n",
        "***Importantly, this was not the only handling strategy that could have been used. I could have keep the data as it was and evaluated the different models to select the most suitable one.***"
      ],
      "metadata": {
        "id": "66Ah1pVXyQDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete task 7"
      ],
      "metadata": {
        "id": "KL-_Nc7bzxtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step was to look at the working class data and how the data is distributed among the unique values. Does it have high cardinality ?\n",
        "\n",
        "Task 8 - Create a graph that shows the value count from the WorkClass column"
      ],
      "metadata": {
        "id": "28G9O21tz3SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Complete task 8"
      ],
      "metadata": {
        "id": "KjHru-PzzqK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of the data is Privately employed. With the data being skewed to the Private sector,the missing data for the working class column was placed into the Private Column.\n",
        "\n",
        "In addition to editing the missing data, a decision was made to combine fields that had a high correlation. The people who never work and the people who are without work were placed in the same category.\n",
        "\n",
        "However the Government job categories remained as the mean salary for the different levels are significantly different.\n",
        "\n",
        "-Federal government - $70,000\n",
        "\n",
        "-State government - $53,180\n",
        "\n",
        "-Local goverment - $47,230\n",
        "(Information retrieved at https://work.chron.com/average-salary-government-employees-7863.html)\n",
        "\n",
        "For the self employed columns, although no additional data was found showing a difference between incorporated self employers and unincorporated self-employees. It was thought that a person would incorporate their company when it grows to a significant size and so there could be a significant difference between the 2 categories. This resulted in the 2 categories being left as they were.\n",
        "\n",
        "Task 9 - Replace the NaN to private within the workclass column and combine the never-worked and without-pay to \"Umemployed\"\n",
        "\n",
        "Display the new data in the same graph as task 7"
      ],
      "metadata": {
        "id": "0EZ3BWXD5PQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for task 9"
      ],
      "metadata": {
        "id": "CbTcL1h151En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An examination of the last column, Occupation, should now happen.\n",
        "\n",
        "Task 10 - Present a graph that shows the value count for the \"Occupation\" column."
      ],
      "metadata": {
        "id": "7mlc0ZEK535n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Task 10"
      ],
      "metadata": {
        "id": "7EvUG6Cb6VDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spread of the data was much more even. 3 options were considered for the missing data. The first option was to create a model that used the other features to predict which occupation the record belonged to. The second option was to delete the data. The last option was to convert the missing data to an intentional out-of-range value. This would allow some of the prediction models to ignore that piece of data.\n",
        "\n",
        "The results of that examination showed that 1843 would still be lost. After creating a model using KNNclasifer, the model came to only a 33% chance of guessing the correct label. Additionally, due to the size of the remaining sample,it was deemed acceptable to lose this number of records.\n",
        "\n",
        "***Here I tried to created a ML classification to predict the most likely country based on the other features. I could have continue with this but 33% acc is very ***\n",
        "\n",
        "Task 11 - Drop the remaining NaN values"
      ],
      "metadata": {
        "id": "JOjuubis6Yr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Task 11"
      ],
      "metadata": {
        "id": "A6mQ1pm763za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Once all the missing data was either replaced or deleted. The other columns were examined to see if the varience in each columns could be reduced.To get a better idea if certain unqiue values could be combined, an examination on the cardinality of the features was conducted\n",
        "\n",
        "Task 12 - Show the number of records that are above and below 50,000"
      ],
      "metadata": {
        "id": "tvO12Ur96-TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete task 12"
      ],
      "metadata": {
        "id": "JpaFi7aH7Ruy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this information, a graph was created from the education column that showed the distribution of the data in each of the eductional categories.\n",
        "\n",
        "Task 13 - Create a graph that shows each of the education values and the number of individuals for over and under 50,000"
      ],
      "metadata": {
        "id": "SH4y1ULR7Xvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete task 13"
      ],
      "metadata": {
        "id": "PqKyzyqvDFNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision was made to combine all categories that were below the High School graduate. As the belief was that anyone who dropped out of highschool wouldn't be using their acdamic ability to aid them in their career.\n",
        "\n",
        "Task 14 - replace the following values\n",
        "\n",
        "[\" 11th\",\" 10th\",\" 7th-8th\",\" 9th\",\" 12th\",\" 5th-6th\",\" 1st-4th\",\" Preschool\"]\n",
        "\n",
        "to dropouts"
      ],
      "metadata": {
        "id": "E9k7rmsADJsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Task 14"
      ],
      "metadata": {
        "id": "Pm09DvvT7JEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the data\n",
        "\n",
        "The data has been cleaned and combined to make it more effective in the modeling process.However, currently there are still a number of columns that have an unsuitable datatype for some of the machine learning algorithms that I will be using.\n",
        "\n",
        "\n",
        "\n",
        "I will be converting all the object datatypes to a numerical type. The options that I have for this are using the Labelencoder and OneHotEncoder.\n",
        "\n",
        "Due to the non-ordinal relations between the data categories in each column. I have chosen to use pandas.get_dummies on the object datatypes -\n",
        "\n",
        " [\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\" were converted to a numeric value.]\n"
      ],
      "metadata": {
        "id": "7-Dk_LZPDxyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "features = df[[\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]]\n",
        "new_features = pd.get_dummies(features)"
      ],
      "metadata": {
        "id": "SyktpmQnJGrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These features are then added to the original dataframe, and the original columns deleted."
      ],
      "metadata": {
        "id": "7rj_caePJO1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace= True)\n",
        "new_features.reset_index(inplace= True)\n",
        "df = pd.merge(df,new_features,on=\"index\",how=\"inner\")\n",
        "df.head(2)\n",
        "df = df.drop([\"index\",\"workclass\",\"education\",\"marital-status\",\"relationship\",\"race\",\"sex\",\n",
        "               \"native-country\",\"occupation\"],1)\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "WmXH0JqzJTt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Continuing with the necessary conversion of the data, the label column was updated so that:\n",
        "\n",
        "0 = Below $50,000\n",
        "\n",
        "1 = Above $50,000\n"
      ],
      "metadata": {
        "id": "fxgQWcNKJaFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"] = df[\"label\"].apply(lambda x : 0 if x ==\" <=50K\" else 1)\n",
        "label_df = df[\"label\"]\n",
        "features = df.drop(\"label\",1)\n",
        "features.head(2)\n",
        "print(features.columns)"
      ],
      "metadata": {
        "id": "bkg6nf26Jh49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling the Data**"
      ],
      "metadata": {
        "id": "scWhJTY-imzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The features were then scaled due to the differences between distributions of the features.\n",
        "\n",
        "*Importantly, I should really prove this by showing the distributions of the different features and discussing how it would impact the different ML models*\n",
        "\n",
        "The MinMaxScaler was used to transforms the data.\n"
      ],
      "metadata": {
        "id": "39lzvj17JmOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(features)\n",
        "scaled_df = scaler.transform(features)"
      ],
      "metadata": {
        "id": "dQ1K2TbeJyPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring the machine learning algorithms**\n",
        "\n",
        "\n",
        "\n",
        "With the transformation of the data complete, a selection of algorithms were chosen to explore the best method to predict the salary classification. Below are the selected algorithms :\n",
        "\n",
        "1)KNeighborsClassifier\n",
        "\n",
        "2)Linear Regression\n",
        "\n",
        "4)Random Forest\n",
        "\n",
        "5)Extra Trees\n",
        "\n",
        "6)Support Vector Machine\n",
        "\n",
        "7)Neural Network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bLyRjXf6J0Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "tqCutE25NCgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data was then divided into test and train datasets. We have 2 ways to divide our data, with the train_test_split option and cross_val_score. We will initially see the results from the test_train_split.\n",
        "\n",
        "Task 14 - Use the test_train_split to create training and testing data"
      ],
      "metadata": {
        "id": "aKTP2Ak7NFgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Task 14"
      ],
      "metadata": {
        "id": "5YH2-bOxNSQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifiers are then set to their default settings and each classifier is trained and tested with the data.\n",
        "\n",
        "Task 15 - For each classifier,  fit the training data, test the model with accuracy_score and the confusion matrix and then save the results to the lists in the code."
      ],
      "metadata": {
        "id": "pOY1rgApNZIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    KNeighborsClassifier(100),\n",
        "    GaussianNB(),\n",
        "    ExtraTreesClassifier(100),\n",
        "    svm.SVC(gamma='scale'),\n",
        "    RandomForestClassifier(100),\n",
        "    MLPClassifier(max_iter=200)\n",
        "\n",
        "    ]\n",
        "\n",
        "alo = []\n",
        "min_max = []\n",
        "standard_list = []\n",
        "confusion_matrix_list = []\n",
        "for clf in classifiers:\n",
        "  name = clf.__class__.__name__\n",
        "  alo.append(name)\n",
        "  # Complete task 15"
      ],
      "metadata": {
        "id": "I1Hvi4bgNbqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This information is placed in a chart so we can compare the accuracy of each classifier."
      ],
      "metadata": {
        "id": "vmx3j_15OKig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(min_max)\n",
        "max_val = max(min_max)\n",
        "print(max_val)\n",
        "labels = list(alo)\n",
        "index = np.arange(1,len(alo)+1)\n",
        "bar_width = 0.35\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "red_counter = False\n",
        "blue_counter = False\n",
        "for i in range(0,len(index)):\n",
        "    if max_val == min_max[i]:\n",
        "        colour = \"Green\"\n",
        "        label = \"Highest Acc Rating\"\n",
        "        ax.bar(index[i] ,min_max[i],bar_width,color=colour , label = label)\n",
        "    elif  min_max[i] > 0.8:\n",
        "        colour = \"r\"\n",
        "        label = \"Above 80% Acc Rating\"\n",
        "        if red_counter == False:\n",
        "            ax.bar(index[i] ,min_max[i],bar_width,color=colour , label = label)\n",
        "            red_counter = True\n",
        "        else:\n",
        "            ax.bar(index[i] ,min_max[i],bar_width,color=colour)\n",
        "\n",
        "    else:\n",
        "        label =\"Below 80% Acc Rating\"\n",
        "        colour = \"b\"\n",
        "        if blue_counter == False:\n",
        "            ax.bar(index[i] ,min_max[i],bar_width,color=colour , label = label)\n",
        "            blue_counter =True\n",
        "        else:\n",
        "            ax.bar(index[i] ,min_max[i],bar_width,color=colour)\n",
        "\n",
        "\n",
        "ax.legend(bbox_to_anchor=(1.3, 0.5))\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "plt.xticks(range(1,len(labels)+1), labels)\n",
        "for index,info in enumerate(ax.get_xticklabels()):\n",
        "    info.set_rotation(90)\n",
        "    ax.text(index+0.90,min_max[index]+0.02,\"{:.2f}\".format(min_max[index]))"
      ],
      "metadata": {
        "id": "6azK-vDPOTQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, apart from Naive Bayes, all the other machine learning algorithms are above 80% with the Random Forest and the Neural Network Classifier slightly edging it at >84 % accuracy. With so many algorithms having a similar accuracy rating, further investigation will be undertaken by looking at the confusion matrix. This will establish where the errors occurred.\n",
        "\n",
        "Have a look at the completed answer tutorial to see how the additional analysis was conducted. In addition, you are welcome to have a look at how I pickled the model and then used in another programming."
      ],
      "metadata": {
        "id": "6tfbMkIGORTk"
      }
    }
  ]
}